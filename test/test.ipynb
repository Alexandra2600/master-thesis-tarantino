{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Notebook\n",
    "\n",
    "This notebook is designed to **repeat the experimental tests** conducted to evaluate the performance of the system.\n",
    "\n",
    "Before running the tests, make sure you have the following prerequisites correctly configured:\n",
    "\n",
    "### Requirements\n",
    "1. **A running instance of a Neo4j database**, either local or remote (e.g. AuraDB).\n",
    "2. **A valid OpenAI API key** to access the language and embedding models.\n",
    "3. **A properly configured `.env` file**, located at the root of the project or in the same folder as this notebook, with the following format:\n",
    "- `NEO4J_URI`\n",
    "- `NEO4J_USERNAME`\n",
    "- `NEO4J_PASSWORD`\n",
    "- `OPENAI_API_KEY`\n",
    "- `ONTOLOGY_FILE=../models/TAMOntology.ttl `\n",
    "\n",
    "### Notebook Structure\n",
    "\n",
    "The notebook is organized into the following main sections:\n",
    "\n",
    "1. **Import and Setup**\n",
    "   - Loads environment variables and dependencies\n",
    "   - Connects to Neo4j and configures the models\n",
    "\n",
    "2. **Knowledge Graph Population**\n",
    "   - Loads user interaction data\n",
    "   - Processes and adds normalized information into the graph\n",
    "   - Resolves entity duplication\n",
    "\n",
    "3. **Evaluation and Testing**\n",
    "   - Loads predefined QA datasets\n",
    "   - Runs tests using both GraphRAG and RAG\n",
    "   - Computes and compares performance metrics \n",
    "\n",
    "---\n",
    "\n",
    "> Run the notebook sequentially to ensure correct state and results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath(\"../src\"))\n",
    "\n",
    "import KG_construction\n",
    "import utils\n",
    "import RAGAS_test\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration: User and Paths\n",
    "Defines the username and base paths for data used throughout the notebook. \\\n",
    "Update these values if you work with different test profiles or directory structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the user to be used in the test\n",
    "user = \"Mateo\"\n",
    "\n",
    "# Path to the data files\n",
    "path_data_profiles = \"./data/profiles/\"  # Synthetic user profiles\n",
    "path_data_qa = \"./data/qa/\"              # QA pairs for evaluation\n",
    "path_data_results = \"./data/results/\"    # Output folder for saving results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knowldege Graph Population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load User Interaction Dataset (CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Construct full path to the user's CSV profile\n",
    "csv_filename = f\"{user}.csv\"\n",
    "csv_path = os.path.join(path_data_profiles, csv_filename)\n",
    "\n",
    "# Check file existence\n",
    "if not os.path.exists(csv_path):\n",
    "    raise FileNotFoundError(f\"CSV not found at: {csv_path}\")\n",
    "\n",
    "# Load the CSV into a DataFrame\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Preview the first few rows\n",
    "print(\"Loaded profile:\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clear the exiting graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Clear the existing graph\n",
    "print(\"Resetting the Knowledge Graph...\")\n",
    "utils.reset_knowledge_graph()\n",
    "print(\"Graph cleared.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process and insert each interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "# Step 2: Process and insert each interaction from the CSV\n",
    "print(\"Processing and inserting user interactions into the graph...\")\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    raw_input = row[\"interaction\"]\n",
    "    reference_date = row[\"date\"]\n",
    "    user_name = row[\"user\"]\n",
    "\n",
    "    processed_input = utils.process_text(\n",
    "        text=raw_input,\n",
    "        current_date=reference_date,\n",
    "        user_name=user_name\n",
    "    )\n",
    "\n",
    "    # Insert into the Knowledge Graph\n",
    "    result = asyncio.run(KG_construction.add_user_input_to_kg(processed_input))\n",
    "    print(f\"Inserted interaction [{index + 1}]: {result}\")\n",
    "\n",
    "# Run entity resolution after bulk insertion\n",
    "asyncio.run(KG_construction.resolve_kg_entities())\n",
    "print(\"Entity resolution completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load QA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load QA dataset\n",
    "qa_filename = user + \"_qa.json\"\n",
    "qa_path = os.path.join(path_data_qa, qa_filename)\n",
    "\n",
    "with open(qa_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    qa_dataset = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(qa_dataset)} QA pairs for evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GraphRAG Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run GraphRAG evaluation\n",
    "print(\"Running evaluation with GraphRAG (graph context)...\")\n",
    "graphRAG_results = RAGAS_test.evaluate_graphRAG(qa_dataset)\n",
    "\n",
    "with open(path_data_results + \"graphRAG_results.csv\", mode='a', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Mateo\", str(graphRAG_results)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run RAG evaluation\n",
    "print(\"Running evaluation with standard RAG (text chunks)...\")\n",
    "rag_results = RAGAS_test.evaluate_RAG(qa_dataset)\n",
    "\n",
    "with open(path_data_results + \"rag_results.csv\", mode='a', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Mateo\", str(rag_results)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display and compare results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert RAGAS results to DataFrames\n",
    "#df_graphRAG = pd.DataFrame(graphRAG_results.scores)\n",
    "#df_RAG = pd.DataFrame(rag_results.scores)\n",
    "\n",
    "# Compute average scores for each metric\n",
    "mean_graphrag = df_graphRAG.mean()\n",
    "mean_rag = df_RAG.mean()\n",
    "\n",
    "# Create a comparison DataFrame\n",
    "comparison_df = pd.DataFrame({\n",
    "    'GraphRAG': mean_graphrag,\n",
    "    'Standard RAG': mean_rag\n",
    "})\n",
    "\n",
    "# Display the average values\n",
    "display(comparison_df.round(4))\n",
    "\n",
    "# Plot comparison as a bar chart\n",
    "ax = comparison_df.plot(kind='bar', figsize=(10, 6), rot=45, color=['#0f8b8d', '#07435d'])\n",
    "\n",
    "plt.title('Average Metric Comparison: GraphRAG vs RAG')\n",
    "plt.ylabel('Score (0–1)')\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.legend(loc='lower right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate Metric Comparison Across Profiles\n",
    "After running the evaluation for each test profile individually the results are saved in CSV files—one for GraphRAG and one for RAG. \n",
    "\n",
    "By reading these CSV files, we can:\n",
    "- load the results for all tested profiles,\n",
    "- **compute the average of each metric across users**, for both GraphRAG and RAG,\n",
    "- **visualize the aggregated comparison** using a grouped bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# === File paths for result CSVs ===\n",
    "CSV_GRAPH = Path(path_data_results + \"graphRAG_results.csv\")\n",
    "CSV_RAG   = Path(path_data_results + \"rag_results.csv\")\n",
    "\n",
    "# === Plot styling ===\n",
    "COLOR_GRAPH = \"#0f8b8d\"\n",
    "COLOR_RAG   = \"#07435d\"\n",
    "BAR_WIDTH   = 0.30\n",
    "\n",
    "# === Metric name mapping for display ===\n",
    "METRIC_LABELS = {\n",
    "    \"llm_context_precision_with_reference\": \"Context precision\",\n",
    "    \"context_recall\":                      \"Context recall\",\n",
    "    \"answer_relevancy\":                    \"Answer relevancy\",\n",
    "    \"faithfulness\":                        \"Faithfulness\",\n",
    "    \"semantic_similarity\":                 \"Answer similarity\",\n",
    "}\n",
    "\n",
    "# === Plot title and config ===\n",
    "TITLE = \"GraphRAG vs RAG\"\n",
    "YLABEL = \"Mean value\"\n",
    "ANNOTATE_BARS = True\n",
    "\n",
    "\n",
    "def load_metrics(csv_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load metrics from a CSV file with rows like:\n",
    "        \"username\",\"{'metricA': 0.5, 'metricB': 0.7, ...}\"\n",
    "    \n",
    "    Returns:\n",
    "        A DataFrame with:\n",
    "            - index = profile names\n",
    "            - columns = individual numeric metrics\n",
    "    \"\"\"\n",
    "    raw = pd.read_csv(\n",
    "        csv_path,\n",
    "        header=None,\n",
    "        names=[\"profile\", \"metrics_str\"],\n",
    "        quotechar='\"',\n",
    "        skipinitialspace=True,\n",
    "        engine=\"python\",\n",
    "    )\n",
    "\n",
    "    # Convert stringified dict into a dictionary, then to columns\n",
    "    expanded = raw[\"metrics_str\"].apply(ast.literal_eval).apply(pd.Series)\n",
    "    expanded.index = raw[\"profile\"]\n",
    "    return expanded\n",
    "\n",
    "\n",
    "def bar_labels(ax, bars):\n",
    "    \"\"\"\n",
    "    Annotate bars with their height values.\n",
    "    \"\"\"\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(\n",
    "            f\"{height:.2f}\",\n",
    "            xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "            xytext=(0, 4),\n",
    "            textcoords=\"offset points\",\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "            fontsize=9,\n",
    "        )\n",
    "        \n",
    "# Load evaluation results\n",
    "df_graph = load_metrics(CSV_GRAPH)\n",
    "df_rag   = load_metrics(CSV_RAG)\n",
    "\n",
    "# Compute average metric scores\n",
    "mean_graph = df_graph.mean().rename(\"GraphRAG\")\n",
    "mean_rag   = df_rag.mean().rename(\"RAG\")\n",
    "\n",
    "# Combine into a comparison DataFrame\n",
    "comparison = pd.DataFrame([mean_graph, mean_rag]).T\n",
    "comparison.index.name = \"Metric\"\n",
    "\n",
    "# Show comparison table\n",
    "print(\"\\nMean metric scores (GraphRAG vs RAG):\")\n",
    "print(comparison.round(4))\n",
    "print()\n",
    "\n",
    "\n",
    "# Apply basic plot styling\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": 11,\n",
    "    \"axes.spines.right\": False,\n",
    "    \"axes.spines.top\":   False,\n",
    "})\n",
    "\n",
    "# Metric selection and label formatting\n",
    "metrics = list(METRIC_LABELS.keys())\n",
    "labels = [METRIC_LABELS[m] for m in metrics]\n",
    "x = np.arange(len(metrics))\n",
    "\n",
    "# Create plot\n",
    "fig, ax = plt.subplots(figsize=(9, 5))\n",
    "\n",
    "# Plot GraphRAG bars\n",
    "bars_graph = ax.bar(\n",
    "    x - BAR_WIDTH / 2,\n",
    "    comparison.loc[metrics, \"GraphRAG\"],\n",
    "    BAR_WIDTH,\n",
    "    label=\"GraphRAG\",\n",
    "    color=COLOR_GRAPH,\n",
    ")\n",
    "\n",
    "# Plot RAG bars\n",
    "bars_rag = ax.bar(\n",
    "    x + BAR_WIDTH / 2,\n",
    "    comparison.loc[metrics, \"RAG\"],\n",
    "    BAR_WIDTH,\n",
    "    label=\"RAG\",\n",
    "    color=COLOR_RAG,\n",
    ")\n",
    "\n",
    "# Configure x-axis\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels, rotation=20, ha=\"right\")\n",
    "\n",
    "# Axis labels and styling\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_title(TITLE, pad=15, weight=\"bold\")\n",
    "ax.set_ylabel(YLABEL)\n",
    "ax.grid(True, axis=\"y\", linestyle=\"--\", linewidth=0.5, alpha=0.7)\n",
    "ax.legend(frameon=False)\n",
    "\n",
    "# Optional: annotate bar values\n",
    "if ANNOTATE_BARS:\n",
    "    bar_labels(ax, list(bars_graph) + list(bars_rag))\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
